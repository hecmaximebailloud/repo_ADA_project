{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527588c8-db2a-4d70-9447-35ddcf7ca0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429ec62-93cb-4216-90d0-bcfba89bc1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of predictions path\n",
    "save_dir_predictions = \"BTC Prices Forecasting\"\n",
    "# plots of sectorial importances path\n",
    "save_dir_sectors_importances = \"Sectors Importance Prices Forecasting\"\n",
    "# plots of features importances path\n",
    "save_dir_features_importances = \"Features Importance Prices Forecasting\"\n",
    "\n",
    "os.makedirs(save_dir_predictions, exist_ok=True)\n",
    "os.makedirs(save_dir_sectors_importances, exist_ok=True)\n",
    "os.makedirs(save_dir_features_importances, exist_ok=True)\n",
    "\n",
    "btc=pd.read_excel(\"btc.xlsx\")\n",
    "all_data=[]\n",
    "all_data.append(btc)\n",
    "\n",
    "# Define start and end dates for the weekly data\n",
    "start_date = pd.to_datetime('09/01/2011')  # Adjusted for weekly data\n",
    "end_date = pd.to_datetime('24/12/2023')    # Adjusted for weekly data\n",
    "\n",
    "# Filter btc data for the specified weekly date range\n",
    "btc_range = btc[(btc['Date'] >= start_date) & (btc['Date'] <= end_date)]\n",
    "btc = btc_range.reset_index(drop=True)  # Reset index without inplace=True\n",
    "all_data[0] = btc.copy()  # Assign a copy of btc to all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2908d27-820f-4e10-9cc3-ba381baa50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_ticker=['AMAZON', 'APPLE', 'google', 'TESLA',\n",
    "                 'GOLD', 'CL1 COMB Comdty', 'NG1 COMB Comdty', 'CO1 COMB Comdty', \n",
    "                 'DowJones', 'Nasdaq', 'S&P', 'Cac40', 'ftse', 'NKY',\n",
    "                 'EURR002W', 'DEYC2Y10', 'USYC2Y10', 'JPYC2Y10', 'TED SPREAD JPN', 'TED SPREAD US', 'TED SPREAD EUR',\n",
    "                 'renminbiusd', 'yenusd', 'eurodollar' ,'gbpusd',\n",
    "                 'active_address_count', 'addr_cnt_bal_sup_10K', 'addr_cnt_bal_sup_100K' , 'miner-revenue-native-unit','miner-revenue-USD','mvrv','nvt','tx-fees-btc', 'tx-fees-usd']\n",
    "                \n",
    "                 \n",
    "                 \n",
    "    \n",
    "\n",
    "all_data_name=['Amazon', 'Apple','Google', 'Tesla',\n",
    "               'Gold', 'WTI Crude Future', 'Natural Gas Future', 'Brent Crude Future',\n",
    "               'DowJones', 'Nasdaq', 'S&P', 'Cac40', 'FTSE', 'Nikkei', \n",
    "               'ECB Policy Rates', 'Yield Curve Germany', 'Yield Curve US', 'Yield Curve Japan', 'TED Spread Japan', 'TED Spread US','TED Spread Europe'\n",
    "               'Renminbi/Dollar', 'Yen/Dollar', 'Euro/Dollar', 'Gbp/Dollar',\n",
    "               'Active Addresses', 'addr_cnt_bal_sup_10K', 'addr_cnt_bal_sup_100K', 'miner-revenue-native-unit', 'miner-revenue-USD', 'mvrv', 'nvt', 'tx-fees-btc', 'tx-fees-usd']\n",
    "               \n",
    "               \n",
    "               \n",
    "\n",
    "\n",
    "for ticker in all_data_ticker:\n",
    "    all_data.append(pd.read_excel(ticker + \".xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc202e-2553-4bcc-af48-2243de9b4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the date and 'raw datas' --> mostly prices \n",
    "keep_columns=['Date','Dernier Prix']\n",
    "\n",
    "# Iterate through all_data and keep only the specified columns\n",
    "for element in all_data:\n",
    "    element = element[keep_columns]\n",
    "# Iterate through all_data and preprocess each element\n",
    "for i, element in enumerate(all_data):\n",
    "    element = element[keep_columns].copy()  # Copy the selected columns\n",
    "    element['Date'] = pd.to_datetime(element['Date'])  # Convert 'Date' column to datetime\n",
    "    # Drop rows with 'Date' values earlier than start_date\n",
    "    element.drop(element[element['Date'] < start_date].index, inplace=True)\n",
    "    all_data[i] = element  # Update all_data with preprocessed element\n",
    "\n",
    "# Find common dates among all datasets\n",
    "common_dates_all = set(btc['Date'])\n",
    "lenmax = len(btc)\n",
    "\n",
    "# Preprocess each dataset to keep only common dates\n",
    "for i, element in enumerate(all_data):\n",
    "    common_dates_all = common_dates_all.intersection(set(element['Date']))\n",
    "    element = element[element['Date'].isin(common_dates_all)].copy()\n",
    "    all_data[i] = element\n",
    "    common_dates_all = set(btc['Date'])  # Reset common_dates_all to original btc dates\n",
    "\n",
    "# Create DataFrame with all unique dates from btc dataset\n",
    "all_dates = pd.DataFrame({'Date': pd.to_datetime(btc['Date'])})\n",
    "\n",
    "# Merge all_dates with dates from other datasets (take into account that all our weekly datasets are indexed with the same dates)\n",
    "for dataset in all_data[1:]:\n",
    "    all_dates = pd.merge(all_dates, dataset[['Date']], on='Date', how='left')\n",
    "\n",
    "# Fill missing values in datasets based on all_dates\n",
    "all_datasets_filled = []\n",
    "all_datasets_filled.append(btc)  # Append original btc dataset\n",
    "for dataset in all_data[1:]:\n",
    "    dataset_filled = pd.merge(all_dates, dataset, on='Date', how='left')  # Merge with all_dates\n",
    "    dataset_filled = dataset_filled.fillna(method='ffill')  # Fill missing values with forward fill\n",
    "    all_datasets_filled.append(dataset_filled)  # Append filled dataset to all_datasets_filled\n",
    "    \n",
    "all_data_ticker=['btc', 'AMAZON', 'APPLE', 'google', 'TESLA',\n",
    "                 'GOLD', 'CL1 COMB Comdty', 'NG1 COMB Comdty', 'CO1 COMB Comdty', \n",
    "                 'DowJones', 'Nasdaq', 'S&P', 'Cac40', 'ftse', 'NKY',\n",
    "                 'EURR002W', 'DEYC2Y10', 'USYC2Y10', 'JPYC2Y10', 'TED SPREAD JPN', 'TED SPREAD US', 'TED SPREAD EUR',\n",
    "                 'renminbiusd', 'yenusd', 'eurodollar' ,'gbpusd',\n",
    "                 'active_address_count', 'addr_cnt_bal_sup_10K', 'addr_cnt_bal_sup_100K' , 'miner-revenue-native-unit','miner-revenue-USD','mvrv','nvt','tx-fees-btc', 'tx-fees-usd']\n",
    "                \n",
    "    \n",
    "# Check if the number of tickers matches the number of DataFrames\n",
    "if len(all_data_ticker) != len(all_datasets_filled):\n",
    "    raise ValueError(\"Number of tickers and number of DataFrames do not match.\")\n",
    "\n",
    "# Assign unique names to columns based on tickers\n",
    "for i, (df, ticker) in enumerate(zip(all_datasets_filled, all_data_ticker)):\n",
    "    df.columns = [f\"{ticker}_{col}\" if col != \"Date\" else \"Date\" for col in df.columns]\n",
    "\n",
    "# Concatenate DataFrames horizontally\n",
    "merged_df = pd.concat(all_datasets_filled, axis=1)\n",
    "merged_df=pd.DataFrame(merged_df)\n",
    "\n",
    "# Extracting columns containing dates and data\n",
    "data_columns = merged_df.iloc[0:, 1::2]  # Selecting every features columns\n",
    "dates_columns = merged_df.iloc[0:, 0] # Selecting one date columns\n",
    "\n",
    "# Creating a DataFrame with only dates and data columns\n",
    "dataset_prices = pd.concat([dates_columns, data_columns], axis=1)\n",
    "dataset_prices = pd.DataFrame(dataset_prices)\n",
    "\n",
    "# Ensure 'Date' column is in datetime format\n",
    "dataset_prices['Date'] = pd.to_datetime(dataset_prices['Date'])\n",
    "\n",
    "# Sort DataFrame by the 'Date' column in ascending order\n",
    "dataset_prices = dataset_prices.sort_values(by='Date')\n",
    "\n",
    "# Resetting index after sorting\n",
    "dataset_prices = dataset_prices.reset_index(drop=True)\n",
    "\n",
    "# Filling missing values by propagating last valid observation forward\n",
    "dataset_prices = dataset_prices.ffill(axis=1)\n",
    "\n",
    "# Setting 'Date' column as index\n",
    "dataset_prices['Date'] = pd.to_datetime(dataset_prices['Date'])\n",
    "dataset_prices.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c193e4e-d971-494d-ad5d-dc5883259658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df):\n",
    "    \n",
    "    # Ensure the DataFrame has a DateTimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be a DateTimeIndex for time series data.\")\n",
    "\n",
    "    # Convert columns to numeric (excluding the 'Date' column)\n",
    "    numeric_cols = df.columns.difference(['Date'])\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Exclude the first row and calculate daily returns for each column\n",
    "    returns_df = pd.DataFrame(index=df.index)  # Create an empty DataFrame with the same index\n",
    "    for col in numeric_cols:\n",
    "        # Calculate returns: (current value / previous value) - 1\n",
    "        returns_df[col + '_returns'] = (df[col] / df[col].shift(1) - 1).replace([np.inf, -np.inf, np.nan], 0)\n",
    "\n",
    "    return returns_df\n",
    "\n",
    "\n",
    "# VOLATILITY (Standard Deviation of a rolling specified window)\n",
    "\n",
    "def calculate_volatility(df, window):\n",
    " \n",
    "    # Convert columns to numeric (excluding the 'Date' column)\n",
    "    numeric_cols = df.columns.difference(['Date'])\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Exclude the first row and calculate volatility for each column using rolling window\n",
    "    volatility_df = pd.DataFrame(index=df.index)  # Create an empty DataFrame with the same index\n",
    "    for col in numeric_cols:\n",
    "        # Calculate volatility using rolling standard deviation\n",
    "        volatility_df[col + '_volatility'] = df[col].rolling(window=window).std()    \n",
    "\n",
    "    # Replace infinite values with NaN and fill NaN with 0\n",
    "    volatility_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    volatility_df.fillna(0, inplace=True)\n",
    "\n",
    "    return volatility_df\n",
    "\n",
    "# Normalized returns (returns / volatility) in order to introduce the volatilities' information to our further analysis\n",
    "\n",
    "def calculate_z_score(returns_df, volatility_df, prices_df):\n",
    "    \n",
    "    # Convert columns to numeric (excluding the 'Date' column)\n",
    "    numeric_cols = prices_df.columns.difference(['Date'])\n",
    "    prices_df[numeric_cols] = prices_df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Calculate z-scores for each column\n",
    "    z_score_df = pd.DataFrame(index=prices_df.index)  # Create an empty DataFrame with the same index\n",
    "    for col in numeric_cols:\n",
    "        # Calculate z-score: returns / volatility\n",
    "        z_score_df[col + '_z_score'] = returns_df[col + '_returns'] / volatility_df[col + '_volatility']\n",
    "    \n",
    "    # Replace infinite values with NaN and fill NaN with 0\n",
    "    z_score_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    z_score_df.fillna(0, inplace=True)\n",
    "\n",
    "    return z_score_df\n",
    "\n",
    "def average(lst):\n",
    "    if len(lst) == 0:\n",
    "        return 0  # Handle the case where the list is empty to avoid division by zero\n",
    "    total = sum(lst)\n",
    "    return total / len(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d00f56-b6cd-41e3-aea8-1cb07ad6ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset containing returns\n",
    "dataset_returns = calculate_returns(dataset_prices)\n",
    "\n",
    "# Dataset containing rolling MONTHLY volatility\n",
    "dataset_volatility=calculate_volatility(dataset_prices, 4) # window of 4 weeks chosen\n",
    "\n",
    "# Dataset containing normalized returns (divided by their volatility)\n",
    "dataset_z_score = calculate_z_score(dataset_returns, dataset_volatility, dataset_prices)\n",
    "\n",
    "# DataFrame with Prices, Returns and Volatilities (non-stationary)\n",
    "dataset_prices_returns=pd.concat([dataset_prices, dataset_returns], axis=1)\n",
    "dataset_prices_returns_volatility=pd.concat([dataset_prices_returns, dataset_volatility], axis=1)\n",
    "dataset_prices_returns_volatility=pd.DataFrame(dataset_prices_returns_volatility)\n",
    "\n",
    "# DataFrame with Returns and Normalized returns (stationary)\n",
    "dataset_returns_zscores=pd.concat([dataset_returns, dataset_z_score], axis=1)\n",
    "dataset_returns_zscores=pd.DataFrame(dataset_returns_zscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5db8ec-0810-4a59-8092-f5e08768f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Tests\n",
    "\n",
    "def adf_test_for_all_features(df):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity for all features in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): The DataFrame containing multiple columns of time series data to be tested.\n",
    "    \n",
    "    Returns:\n",
    "    - stationary_series (list): A list containing names of stationary series.\n",
    "    - non_stationary_series (list): A list containing names of non-stationary series.\n",
    "    \"\"\"\n",
    "  \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        # Perform ADF test for the current column\n",
    "        result = adfuller(df[col])\n",
    "        p_value = result[1]\n",
    "        \n",
    "        # Determine if the series is stationary based on p-value\n",
    "        if p_value <= 0.05:\n",
    "            print(\"Stationary -> \", col)\n",
    "        else:\n",
    "            print(\"Non Stationary -> \", col)\n",
    "\n",
    "\n",
    "\n",
    "adf_test_for_all_features(dataset_prices_returns_volatility)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7328d0-b653-46fe-a8f9-5accc36d9671",
   "metadata": {},
   "source": [
    "### Correaltion between all the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e0cf6-81a9-46f4-9737-9077a490f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset choice (note that the dataset chosen must be stationary):\n",
    "    \n",
    "# dataset_prices\n",
    "# dataset_returns\n",
    "# dataset_volatility\n",
    "# dataset_z_score\n",
    "# dataset_prices_returns_volatility\n",
    "# dataset_returns_zscores\n",
    "\n",
    "correlation_matrix = dataset_returns.corr()\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "plt.figure(figsize=(60, 60))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='magma', fmt=\".2f\", linewidths=.5, linecolor = 'white' )\n",
    "plt.title('Correlation matrix of the WEEKLY features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095de06-6277-4d0a-867c-784f37c0c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are working with stationary and non stationary datas to predict BTC price.\n",
    "# I will use the dataset : dataset_prices_returns_volatility\n",
    "# Which contains raw datas, returns and volatilities                              \n",
    "\n",
    "# Droping labels containing the BTC features (prices, returns, volatilities)\n",
    "features1 = dataset_prices_returns_volatility.drop(\"btc_Dernier Prix\", axis=1)\n",
    "features2 = features1.drop(\"btc_Dernier Prix_returns\", axis=1)\n",
    "features = features2.drop(\"btc_Dernier Prix_volatility\", axis=1)\n",
    "\n",
    "labels = dataset_prices_returns_volatility[\"btc_Dernier Prix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54fb20-5c15-4ffe-9354-ac3b72596e60",
   "metadata": {},
   "source": [
    "### Grid Search with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643037b9-42b7-43a4-bb20-30948aa69c1f",
   "metadata": {},
   "source": [
    "# Define the parameter grid for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100 ],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  \n",
    "    cv=5,  \n",
    "    verbose=1,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "grid_search.fit(features, labels)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f532753-4b4c-4475-b2f8-f4dfb4826fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100, 200],  \n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,  # Number of parameter settings sampled (100 is just an example)\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the data\n",
    "random_search.fit(features, labels)  # Assume 'features' and 'labels' are already defined\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afff7f2-41d7-4f59-9278-31a6116659f4",
   "metadata": {},
   "source": [
    "### Random Forest with best hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c9daf-c307-4d8c-9142-501cad959223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically, I chose a 200 weeks window analysis. Which is representative to a Bitcoin cycle period.\n",
    "# I take 160 weeks to TRAIN my model and 40 weeks to TEST it.\n",
    "# Then I make my Random Forest predictions on the price of Bitcoin and iterate with a 4 weeks rolling.\n",
    "# I chose to make a rolling incrementation instead of an expanding one in order to keep only \n",
    "# the dynamics that might explain the Bitcoin's narratives (features that explain Bitcoin are evolving with time !)\n",
    "\n",
    "\n",
    "all_total_prices = []\n",
    "bitcoin_prices = []\n",
    "accuracy_values = []\n",
    "group_importances_mean = []\n",
    "\n",
    "\n",
    "for i in range(0, len(labels)-200, 4):\n",
    " \n",
    "    train_labels=labels.iloc[0+i:160+i]\n",
    "    train_features=features.iloc[0+i:160+i]\n",
    "\n",
    "    test_labels=labels.iloc[159+i:200+i]\n",
    "    test_features=features.iloc[159+i:200+i]\n",
    "    \n",
    "    train_dates=dataset_prices.index[0+i:160+i]\n",
    "    test_dates=dataset_prices.index[159+i:200+i]\n",
    "\n",
    "\n",
    "    # Instantiate model \n",
    "    rf = RandomForestRegressor(n_estimators= 25, max_depth=10, min_samples_leaf=2, min_samples_split=2, random_state=(42))\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels)\n",
    "\n",
    "    # Make Predictions on Test Data\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = rf.predict(test_features)\n",
    "\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - test_labels)\n",
    "\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / test_labels)\n",
    "\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_values.append(accuracy)\n",
    "\n",
    "    # Variable Importances\n",
    "\n",
    "    # Get numerical feature importances\n",
    "    importances = list(rf.feature_importances_)\n",
    "    \n",
    "\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(list(features.columns), importances)]\n",
    "\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    feature_importances_df = pd.DataFrame(feature_importances[:10], columns=['Feature', 'Importance'])\n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    #packages of features (sectorials groups)\n",
    "\n",
    "\n",
    "    # Feature groups defined earlier\n",
    "    feature_groups = {\n",
    "        \"Equity_Indices\": [\n",
    "            \"DowJones\", \"Nasdaq\", \"S&P\", \"Cac40\", \"ftse\", \"NKY\"\n",
    "        ],\n",
    "        \"Individual_Stocks\": [\n",
    "            \"AMAZON\", \"APPLE\", \"google\", \"TESLA\"\n",
    "        ],\n",
    "        \"Commodity_Prices\": [\n",
    "            \"GOLD\", \"CL1 COMB Comdty\", \"NG1 COMB Comdty\", \"CO1 COMB Comdty\"\n",
    "        ],\n",
    "        \"Interest_Rates_Yields\": [\n",
    "            \"USYC2Y10\", \"TED SPREAD EUR\", \"TED SPREAD US\", \"TED SPREAD JPN\", \"EURR002W\", \"JPYC2Y10\",\"DEYC2Y10\"\n",
    "        ],\n",
    "       \n",
    "        \"Blockchain_Cryptocurrency_Metrics\": [\n",
    "            \"active_address_count\", \"addr_cnt_bal_sup_10K\", \"addr_cnt_bal_sup_100K\", \"miner-revenue-native-unit\", \"miner-revenue-USD\", \"mvrv\", \"nvt\", \"tx-fees-btc\", \"tx-fees-usd\"\n",
    "        ],\n",
    "        \"Foreign_Exchange_Rates\": [\n",
    "            \"eurodollar\", \"gbpusd\", \"renminbiusd\", \"yenusd\"\n",
    "        ]\n",
    "         }\n",
    "\n",
    "\n",
    "    # Initialize a dictionary to hold the aggregated importances for each group\n",
    "    group_importances = {group: 0 for group in feature_groups}\n",
    "\n",
    "    # Iterate over each feature importance returned by the model\n",
    "    for feature, importance in zip(features.columns, rf.feature_importances_):\n",
    "        # Initialize a variable to determine if the feature was found in any group\n",
    "        found_group = None\n",
    "        \n",
    "        # Check if the feature name contains any of the group identifiers\n",
    "        for group, group_features in feature_groups.items():\n",
    "            if any(group_feature in feature for group_feature in group_features):\n",
    "                found_group = group\n",
    "                break\n",
    "        \n",
    "        # Aggregate the importance to the respective group\n",
    "        if found_group:\n",
    "            group_importances[found_group] += importance\n",
    "        else:\n",
    "            print(f\"Feature '{feature}' not clearly assignable to any group\")\n",
    "\n",
    "    # Sort the group importances by most important first\n",
    "    sorted_group_importances = sorted(group_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Convert to DataFrame for easier plotting\n",
    "    group_importances_df = pd.DataFrame(sorted_group_importances, columns=['Group', 'Importance'])\n",
    "    group_importances_mean.append(group_importances_df)\n",
    "    # Define the plot title and the filename for saving the plot\n",
    "\n",
    "   ############################################################################################## \n",
    "\n",
    "    all_total_prices.append(np.mean(predictions[-4:])) # KEEP THE LAST 4 predicted weekly returns --> last month returns prediction\n",
    "    bitcoin_prices.append(np.mean(test_labels[-4:]))\n",
    "    \n",
    "\n",
    "all_total_returns = pd.DataFrame(all_total_prices)\n",
    "bitcoin_returns = pd.DataFrame(bitcoin_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732315f-5804-450e-8b5b-6ea90f49047d",
   "metadata": {},
   "source": [
    "### Predictions' accuracy over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be9d84-9b53-45f4-a3cd-3c35154980b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_accuracy = pd.to_datetime('21/09/2014')  # Adjusted for weekly data\n",
    "end_date_accuracy = pd.to_datetime('24/12/2023')    # Adjusted for weekly data\n",
    "dates_accuracy = pd.date_range(start=start_date_accuracy, end=end_date_accuracy, freq='M')\n",
    "\n",
    "dates_accuracy_fit = pd.to_numeric(dates_accuracy)\n",
    "coefficients = np.polyfit(dates_accuracy_fit, accuracy_values, deg=1)  \n",
    "\n",
    "dates_fit = np.linspace(min(dates_accuracy_fit), max(dates_accuracy_fit), 100)  \n",
    "dates_fit_datetime = pd.to_datetime(dates_fit) \n",
    "\n",
    "accuracy_values_fit = np.polyval(coefficients, dates_fit)\n",
    "\n",
    "accuracy_values_mean = average(accuracy_values)\n",
    "print(\"Accuracy of the predicted prices with all features:\", round(accuracy_values_mean,2),\"%.\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(dates_accuracy[:len(accuracy_values)], accuracy_values, label='Original Accuracy')\n",
    "plt.plot(dates_fit_datetime[:len(accuracy_values_fit)], accuracy_values_fit, color='red', label='Accuracy Fitted Line')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Curve Fitting with Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff623d49-a130-4a3e-83b2-b62d0749ca95",
   "metadata": {},
   "source": [
    "### Group's Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903f859-8d62-4601-aa5b-43e5e1684aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groups = pd.concat(group_importances_mean)\n",
    "\n",
    "grouped_groups_mean = all_groups.groupby('Group').mean({'Importance': 'sum'}).sort_values(by='Importance', ascending = False)\n",
    "\n",
    "grouped_groups_mean.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.pie(\n",
    "    grouped_groups_mean['Importance'], \n",
    "    labels=grouped_groups_mean['Group'], \n",
    "    autopct='%1.1f%%',  \n",
    "    startangle=180  \n",
    ")\n",
    "plt.axis('equal')  \n",
    "plt.title('Average Group Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d290dc-c324-4262-8d1f-62c7b28a9742",
   "metadata": {},
   "source": [
    "### Evolution of the Importance of groups over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cfbe0-d30c-4c04-8fc5-f4e7e3c92e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_importance = pd.to_datetime('21/09/2014')  # Adjusted for weekly data\n",
    "end_date_importance = pd.to_datetime('24/12/2023')    # Adjusted for weekly data\n",
    "dates_importance = pd.date_range(start=start_date_importance, end=end_date_importance, freq='M')\n",
    "\n",
    "for df, date in zip(group_importances_mean, dates_importance):\n",
    "    df['Date'] = pd.to_datetime(date)\n",
    "\n",
    "all_data = pd.concat(group_importances_mean)\n",
    "\n",
    "filtered_data = all_data[all_data['Group'].isin(['Blockchain_Cryptocurrency_Metrics', 'Equity_Indices'])]\n",
    "\n",
    "pivot_data = filtered_data.pivot(index='Date', columns='Group', values='Importance')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pivot_data) \n",
    "plt.title(\"Evolution of the 2 most important groups\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Importance (%)')\n",
    "plt.grid(True)\n",
    "plt.legend(labels = ['Blockchain_Cryptocurrency_Metrics', 'Equity_Indices'], title='Groups', bbox_to_anchor=(0.5, -0.3), loc='lower center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26671031-888c-4c25-8e3d-3063a599ee3b",
   "metadata": {},
   "source": [
    "### ACF and PACF model to identify SARIMA optimal parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e42ee-f8f2-4976-85ba-a27ffaedf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_prices = dataset_prices['btc_Dernier Prix'].dropna()\n",
    "\n",
    "\n",
    "# Plot ACF to determine q\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(btc_prices, lags=40, ax=plt.gca())\n",
    "plt.title('Autocorrelation (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF to determine p\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(btc_prices, lags=40, ax=plt.gca())\n",
    "plt.title('Partial Autocorrelation (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3184d-3afa-4b35-8e31-ac7025562da3",
   "metadata": {},
   "source": [
    "### Testing the seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9abb5d-165b-431f-8a5e-088a1213961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(btc_prices, model='additive', period=52)  \n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23544b8f-9a78-42a1-af99-fb47954780eb",
   "metadata": {},
   "source": [
    "### SARIMA Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8d870-7254-45b7-adf7-254385c19602",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_model = SARIMAX(btc_prices, order=(2, 1, 2), seasonal_order=(2, 1, 2, 52)) # annual seasonality with weekly datas.  \n",
    "sarima_result = sarima_model.fit()\n",
    "forecast = sarima_result.forecast(steps=104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3665901-2a27-4c4d-9acd-0a3d7c097c7b",
   "metadata": {},
   "source": [
    "### Plot of the forecasted prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f398a32-fc6f-497d-8230-e5129072bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_steps = 104 # 2 years of forecasts \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(btc_prices, label='Historical Prices')\n",
    "plt.plot(pd.date_range(btc_prices.index[-1], periods=forecast_steps, freq='W'), forecast, label='Forecasted Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b43c6-1fbc-42a4-aa02-b97451800e3c",
   "metadata": {},
   "source": [
    "### Save paths for new predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58148a-9fe2-4606-8671-914d01d83f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of predictions path\n",
    "save_dir_predictions_rfe = \"BTC Prices Forecasting RFE\"\n",
    "# plots of sectorial importances path\n",
    "save_dir_sectors_importances_rfe = \"Sectors Importance Prices Forecasting RFE\"\n",
    "# plots of features importances path\n",
    "save_dir_features_importances_rfe = \"Features Importance Prices Forecasting RFE\"\n",
    "\n",
    "os.makedirs(save_dir_predictions_rfe, exist_ok=True)\n",
    "os.makedirs(save_dir_sectors_importances_rfe, exist_ok=True)\n",
    "os.makedirs(save_dir_features_importances_rfe, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce48ff-dbbd-458b-a667-c27e3c612cb1",
   "metadata": {},
   "source": [
    "### Selection of the most explicative features using Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86d55e-0214-4498-81c8-ae30e3d6c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of separating the features and target variable (assuming dataset_prices is ready)\n",
    "features_rfe = dataset_prices.drop(columns=['btc_Dernier Prix'])  # Adjust the target column name\n",
    "target_rfe = dataset_prices['btc_Dernier Prix']\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize a model for feature ranking\n",
    "model = RandomForestRegressor(n_estimators=25, random_state=42)\n",
    "\n",
    "# Initialize Recursive Feature Elimination with the number of features to select\n",
    "rfe = RFE(model, n_features_to_select=5)  # Adjust `n_features_to_select`\n",
    "\n",
    "# Fit the RFE model to the data\n",
    "rfe.fit(features_rfe, target_rfe)\n",
    "\n",
    "# Get the rankings of features (1 = most important)\n",
    "ranking = rfe.ranking_\n",
    "selected_features = features_rfe.columns[rfe.support_]\n",
    "print(\"Selected Features via RFE:\", selected_features)\n",
    "\n",
    "# Apply the selection to your feature set\n",
    "features_selected = features_rfe[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57fb8e-85eb-46fb-9d7a-9147dbf64b00",
   "metadata": {},
   "source": [
    "# Define the parameter grid for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100 ],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  \n",
    "    cv=5,  \n",
    "    verbose=1,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "grid_search.fit(features_selected, target_rfe)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041235fc-f11b-431d-86bb-ff54efc2125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100, 200],  # Expanded range\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,  # Number of parameter settings sampled (100 is just an example)\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the data\n",
    "random_search.fit(features_selected, target_rfe)  # Assume 'features' and 'labels' are already defined\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6d1d5-d7c6-43ab-8dd9-3ec30d36630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.savefig(os.path.join(save_dir_sectors_importances_rfe, plot_title_grouped_rfe))\n",
    "# plot_path = os.path.join(save_dir_features_importances_rfe, plot_title)\n",
    "# plt.savefig(plot_path)\n",
    "# plot_title_rfe = f\"Bitcoin_Prices_Predictions_{test_dates[0]}_to_{test_dates[-1]}.png\"\n",
    "# plot_path = os.path.join(save_dir_predictions_rfe, plot_title_rfe)\n",
    "# plt.savefig(plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6de06-f0cb-4ca7-8098-03ef47eb7c9d",
   "metadata": {},
   "source": [
    "### Random Forest using the new selected features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ac1ab-de37-4e04-a8aa-d0ccc5275977",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_total_prices_rfe = []\n",
    "bitcoin_prices_rfe = []\n",
    "accuracy_values_rfe = []\n",
    "group_importances_mean_rfe = []\n",
    "\n",
    "\n",
    "for i in range(0, len(target_rfe)-200, 4):\n",
    " \n",
    "    train_labels=target_rfe.iloc[0+i:160+i]\n",
    "    train_features=features_selected.iloc[0+i:160+i]\n",
    "\n",
    "    test_labels=target_rfe.iloc[159+i:200+i]\n",
    "    test_features=features_selected.iloc[159+i:200+i]\n",
    "    \n",
    "    train_dates=dataset_prices.index[0+i:160+i]\n",
    "    test_dates=dataset_prices.index[159+i:200+i]\n",
    "    \n",
    "    print(\"\\nStart Train : \", train_dates[0])\n",
    "    print(\"End Train : \", train_dates[-1])\n",
    "    print(\"Start Test : \", test_dates[0])\n",
    "    print(\"End Test : \", test_dates[-1])\n",
    "\n",
    "    # Instantiate model \n",
    "    rf = RandomForestRegressor(n_estimators= 100, max_depth=30, min_samples_leaf=2, min_samples_split=2, random_state=(42))\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels)\n",
    "\n",
    "    # Make Predictions on Test Data\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions_rfe = rf.predict(test_features)\n",
    "\n",
    "    # Calculate the absolute errors\n",
    "    errors_rfe = abs(predictions_rfe - test_labels)\n",
    "\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape_rfe = 100 * (errors_rfe / test_labels)\n",
    "\n",
    "    # Calculate and display accuracy\n",
    "    accuracy_rfe = 100 - np.mean(mape_rfe)\n",
    "    print('Accuracy:', round(accuracy_rfe, 2), '%.')\n",
    "    accuracy_values_rfe.append(accuracy_rfe)\n",
    "\n",
    "    # Variable Importances\n",
    "\n",
    "    # Get numerical feature importances\n",
    "    importances_rfe = list(rf.feature_importances_)\n",
    "    \n",
    "\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(list(features_selected.columns), importances_rfe)]\n",
    "\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    feature_importances_df = pd.DataFrame(feature_importances[:10], columns=['Feature', 'Importance'])\n",
    "    print(feature_importances_df)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # list of x locations for plotting\n",
    "    x_values = list(range(len(importances_rfe)))\n",
    "    # Make a bar chart\n",
    "    plt.bar(feature_importances_df['Feature'], feature_importances_df['Importance'], color = 'grey') \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Axis labels and title\n",
    "    plt.ylabel('Importance'); plt.xlabel('Features'); plt.title('Variable Importances'); \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Saving plots of Features Importances (COMMENT THE 3 NEXT LINES IF YOU DON'T WANT TO SAVE THESE PLOT !)\n",
    "    plot_title = f\"Features_Importances_{test_dates[0].strftime('%Y-%m-%d')}_{test_dates[-1].strftime('%Y-%m-%d')}.png\"\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    #packages of features (sectorials groups)\n",
    "\n",
    "\n",
    "    # Feature groups defined earlier according to features selected. \n",
    "    feature_groups = {\n",
    "        \"Equity_Indices\": [\n",
    "             \"Nasdaq\", \"S&P\"\n",
    "        ],\n",
    "        \"Individual_Stocks\": [\n",
    "            \"google\", \"TESLA\"\n",
    "        ],\n",
    "       \n",
    "        \"Blockchain_Cryptocurrency_Metrics\": [\n",
    "             \"miner-revenue-USD\"\n",
    "        ],\n",
    "        \n",
    "         }\n",
    "\n",
    "\n",
    "    # Initialize a dictionary to hold the aggregated importances for each group\n",
    "    group_importances = {group: 0 for group in feature_groups}\n",
    "\n",
    "    # Iterate over each feature importance returned by the model\n",
    "    for feature, importance in zip(features_selected.columns, rf.feature_importances_):\n",
    "        # Initialize a variable to determine if the feature was found in any group\n",
    "        found_group = None\n",
    "        \n",
    "        # Check if the feature name contains any of the group identifiers\n",
    "        for group, group_features in feature_groups.items():\n",
    "            if any(group_feature in feature for group_feature in group_features):\n",
    "                found_group = group\n",
    "                break\n",
    "        \n",
    "        # Aggregate the importance to the respective group\n",
    "        if found_group:\n",
    "            group_importances[found_group] += importance\n",
    "        else:\n",
    "            print(f\"Feature '{feature}' not clearly assignable to any group\")\n",
    "\n",
    "    # Sort the group importances by most important first\n",
    "    sorted_group_importances = sorted(group_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Convert to DataFrame for easier plotting\n",
    "    group_importances_df = pd.DataFrame(sorted_group_importances, columns=['Group', 'Importance'])\n",
    "    group_importances_mean_rfe.append(group_importances_df)\n",
    "    # Define the plot title and the filename for saving the plot\n",
    "    plot_title_grouped_rfe = f\"Sectors_Importances_{test_dates[0].strftime('%Y-%m-%d')}_{test_dates[-1].strftime('%Y-%m-%d')}.png\"\n",
    "    \n",
    "    \n",
    "    # Printing and plotting the grouped feature importances\n",
    "    print(group_importances_df)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(group_importances_df['Group'], group_importances_df['Importance'], color ='darkblue')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Feature Groups')\n",
    "    plt.title('Grouped Variable Importances')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Saving plots of Sectors Importances (COMMENT THE NEXT LINE IF YOU DON'T WANT TO SAVE THESE PLOT !)\n",
    "    plt.show()\n",
    "\n",
    "   ############################################################################################## \n",
    "    \n",
    "    \n",
    "    # Predictions VS Actual Prices\n",
    "\n",
    "    train_dates = dataset_prices.index[0+i:160+i]\n",
    "    test_dates = dataset_prices.index[159+i:200+i]\n",
    "\n",
    "    # Convert datetime64[ns] to matplotlib's datetime format\n",
    "    train_dates = [date.strftime('%Y-%m-%d') for date in train_dates]\n",
    "\n",
    "    test_dates = [date.strftime('%Y-%m-%d') for date in test_dates]\n",
    "    \n",
    "    plt.style.use(\"bmh\")\n",
    "\n",
    "    # Plotting the entire dataset\n",
    "    plt.figure(figsize=(26, 18))\n",
    "    \n",
    "\n",
    "\n",
    "    # Plotting training set\n",
    "    plt.plot(train_dates, train_labels, label = 'Training Set (Actual)', color='black', alpha=0.7)\n",
    "\n",
    "    # Plotting testing set\n",
    "    plt.plot(test_dates, test_labels, label = 'Testing Set (Actual)', color='green', alpha=0.7)\n",
    "\n",
    "    # Plotting predictions on the testing set\n",
    "    plt.plot(test_dates, predictions_rfe, label = 'Testing Set (Predicted)', color='red', alpha=0.7)\n",
    "\n",
    "\n",
    "    plt.title('Bitcoin Prices - Actual vs Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Bitcoin Price')\n",
    "    plt.legend()\n",
    "    \n",
    "  \n",
    "    # Set the locator for x-axis ticks to display every 6 months\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=15))\n",
    "    \n",
    "    # Save the plot\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "    all_total_prices_rfe.append(np.mean(predictions_rfe[-4:])) # KEEP THE LAST 4 predicted weekly returns --> last month returns prediction\n",
    "    bitcoin_prices_rfe.append(np.mean(test_labels[-4:]))\n",
    "    \n",
    "\n",
    "all_total_returns_rfe = pd.DataFrame(all_total_prices_rfe)\n",
    "bitcoin_returns_rfe = pd.DataFrame(bitcoin_prices_rfe)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fddad-3441-460a-a07c-81acdd02b453",
   "metadata": {},
   "source": [
    "### Predictions Accuracy with selected features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd0bfc-462b-443e-8276-3bebeffae6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_accuracy = pd.to_datetime('21/09/2014')  # Adjusted for weekly data\n",
    "end_date_accuracy = pd.to_datetime('24/12/2023')    # Adjusted for weekly data\n",
    "dates_accuracy = pd.date_range(start=start_date_accuracy, end=end_date_accuracy, freq='M')\n",
    "\n",
    "dates_accuracy_fit = pd.to_numeric(dates_accuracy)\n",
    "coefficients = np.polyfit(dates_accuracy_fit, accuracy_values_rfe, deg=1)  # Adjust the degree as needed\n",
    "\n",
    "# Generate dates for the fitted curve\n",
    "dates_fit = np.linspace(min(dates_accuracy_fit), max(dates_accuracy_fit), 100)  # Generate dates for the fitted curve\n",
    "dates_fit_datetime = pd.to_datetime(dates_fit)  # Convert numeric dates back to datetime\n",
    "\n",
    "# Evaluate the polynomial at the dates for the fitted curve\n",
    "accuracy_values_fit = np.polyval(coefficients, dates_fit)\n",
    "\n",
    "accuracy_values_mean_rfe = average(accuracy_values_rfe)\n",
    "print(\"Accuracy of the predicted prices after RFE:\",round(accuracy_values_mean_rfe,2),\"%.\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Plot the original data and the fitted curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(dates_accuracy[:len(accuracy_values_rfe)], accuracy_values_rfe, label='Original Accuracy')\n",
    "plt.plot(dates_fit_datetime[:len(accuracy_values_fit)], accuracy_values_fit, color='red', label='Accuracy Fitted Curve')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Curve Fitting with Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ddb33-7727-4de1-80b9-b5b326ba6919",
   "metadata": {},
   "source": [
    "### Comparison between actual BTC prices, predicted prices (all features) and predicted prices (10 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97399ce5-0e7b-49b9-93c1-e7d19ec602ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_comparison = pd.to_datetime('21/09/2014')  # Adjusted for weekly data\n",
    "end_date_comparison = pd.to_datetime('24/12/2023')    # Adjusted for weekly data\n",
    "dates_comparison = pd.date_range(start=start_date_comparison, end=end_date_comparison, freq='M')\n",
    "\n",
    "# Plot the original data and the fitted curve\n",
    "plt.figure(figsize = (30, 12))\n",
    "plt.plot(dates_comparison[:len(bitcoin_returns_rfe)], bitcoin_returns_rfe, color = 'black', label = 'Actual Bitcoin Price', alpha = 0.3)\n",
    "plt.plot(dates_comparison[:len(all_total_returns_rfe)], all_total_returns_rfe, color = 'red', label = 'Predicted Bitcoin Price after RFE')\n",
    "plt.plot(dates_comparison[:len(all_total_returns)], all_total_returns, color = 'blue', label = 'Predicted Bitcoin Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Predicted (RFE/without) VS Actual Bitcoin Monthly Prices')\n",
    "plt.legend(fontsize = 20)\n",
    "plt.grid(True, color='grey', linewidth='0.5', linestyle='-')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd5323-e3d9-4a7b-9f96-8046ef62de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_model = SARIMAX(all_total_returns_rfe, order=(2, 1, 2), seasonal_order=(2, 1, 2, 52)) # annual seasonality with weekly datas.  \n",
    "sarima_result = sarima_model.fit()\n",
    "\n",
    "forecast_rfe = sarima_result.forecast(steps=104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35220e-43cb-47cd-a3f2-61186ceb6902",
   "metadata": {},
   "source": [
    "sarima_model = SARIMAX(all_total_returns_rfe, order=(2, 1, 2), seasonal_order=(2, 1, 2, 52)) # annual seasonality with weekly datas.  \n",
    "sarima_result = sarima_model.fit()\n",
    "print(sarima_result.summary())\n",
    "\n",
    "forecast = sarima_result.forecast(steps=104)\n",
    "print(\"SARIMA Forecast:\", forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79526fbf-0eca-4418-a251-118a2ada2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_steps = 104 # 2 years of forecasts \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(btc_prices, label='Historical Prices')\n",
    "plt.plot(pd.date_range(btc_prices.index[-1], periods=forecast_steps, freq='W'), forecast_rfe, label='Forecasted Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a7cf2-764a-471a-8912-c8fd32815732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = pd.DataFrame(forecast)\n",
    "df_forecast_rfe = pd.DataFrame(forecast_rfe)\n",
    "start_date_comparison = pd.to_datetime('31/12/2023')  # Adjusted for weekly data\n",
    "# Create a date range\n",
    "date_range = pd.date_range(start=start_date_comparison, periods=len(df_forecast_rfe), freq= 'W')\n",
    "\n",
    "# Set the date range as the index of the DataFrame\n",
    "df_forecast_rfe.index = date_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ac5ff-d714-410d-8416-8688f2e30f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_rfe = returns_forecast_rfe.std()*np.sqrt(52)\n",
    "print(volatility_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058e133-ffbb-4a93-866e-8026462c5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_forecast_rfe = calculate_returns(df_forecast_rfe)\n",
    "vol_forecast_rfe = calculate_volatility(df_forecast_rfe,4)\n",
    "returns_forecast = calculate_returns(df_forecast)\n",
    "vol_forecast = calculate_volatility(df_forecast,4)\n",
    "print(returns_forecast_rfe)\n",
    "print(vol_forecast_rfe)\n",
    "print(returns_forecast)\n",
    "print(vol_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855eca05-fb9c-4685-a32d-3fb19c4a3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(returns_forecast_rfe, label='returns of the forecasted price using RFE predicted prices')\n",
    "plt.plot(returns_forecast, label='Returns of the forecasted prices using actual bitcoin prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Returns')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57161a9-dc0e-4aa7-9fd6-3016e7234dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(vol_forecast_rfe, label='Volatility of the forecasted price using RFE predicted prices')\n",
    "plt.plot(vol_forecast, label='Volatility of the forecasted prices using actual bitcoin prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674671a3-2134-45b4-ad34-70dfc9c78301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2ef85-ffb3-466f-bc5a-31d7901ff167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
